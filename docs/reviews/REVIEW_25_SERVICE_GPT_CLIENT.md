# Service GPT Client ê²€í†  ë³´ê³ ì„œ

## ê²€í†  ëŒ€ìƒ
- íŒŒì¼: `src/services/gpt_client.py`
- ê²€í†  ì¼ì: 2024ë…„
- ê²€í†  ë²”ìœ„: OpenAI API í˜¸ì¶œ, ì¬ì‹œë„ ë¡œì§, ì—ëŸ¬ ì²˜ë¦¬, í† í° ê´€ë¦¬

---

## âœ… ì •ìƒ ë™ì‘ ë¶€ë¶„

### 1. ì¬ì‹œë„ ë¡œì§ (Lines 42-86)
```python
def _retry_with_backoff(self, func, *args, **kwargs):
    """ì§€ìˆ˜ ë°±ì˜¤í”„ë¥¼ ì‚¬ìš©í•œ ì¬ì‹œë„ ë¡œì§"""
    for attempt in range(self.max_retries):
        try:
            return func(*args, **kwargs)
        except RateLimitError as e:
            wait_time = self.retry_delay * (2 ** attempt)
            time.sleep(wait_time)
        except (APIConnectionError, APITimeoutError) as e:
            wait_time = self.retry_delay * (2 ** attempt)
            time.sleep(wait_time)
```
- âœ… ì§€ìˆ˜ ë°±ì˜¤í”„ ì „ëµ ì‚¬ìš©
- âœ… RateLimitError, APIConnectionError, APITimeoutError êµ¬ë¶„ ì²˜ë¦¬
- âœ… ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì œí•œ

### 2. ì»¤ìŠ¤í…€ ì˜ˆì™¸ ì‚¬ìš© (Line 80)
```python
raise GPTAPIError(f"API ì˜¤ë¥˜: {str(e)}", status_code=getattr(e, 'status_code', None))
```
- âœ… ì»¤ìŠ¤í…€ ì˜ˆì™¸ë¡œ ì¼ê´€ëœ ì—ëŸ¬ ì²˜ë¦¬
- âœ… status_code ë³´ì¡´

### 3. ì‘ë‹µ íŒŒì‹± (Lines 119-130)
```python
result = {
    "content": response.choices[0].message.content,
    "role": response.choices[0].message.role,
    "usage": {
        "prompt_tokens": response.usage.prompt_tokens,
        "completion_tokens": response.usage.completion_tokens,
        "total_tokens": response.usage.total_tokens
    },
    "model": response.model,
    "finish_reason": response.choices[0].finish_reason
}
```
- âœ… êµ¬ì¡°í™”ëœ ì‘ë‹µ ë°˜í™˜
- âœ… í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ í¬í•¨

### 4. ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (Line 194)
```python
gpt_client = GPTClient()
```
- âœ… ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš©

### 5. ì—°ê²° í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ (Lines 174-190)
```python
def test_connection(self) -> bool:
    """API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        response = self.chat_completion(
            messages=[{"role": "user", "content": "test"}],
            max_tokens=5
        )
        return True
    except Exception as e:
        return False
```
- âœ… í—¬ìŠ¤ì²´í¬ ë©”ì„œë“œ ì œê³µ

---

## âš ï¸ ë°œê²¬ëœ ë¬¸ì œì 

### 1. ì‘ë‹µ ì¸ë±ì‹± ì•ˆì „ì„± ë¶€ì¡± (Lines 121-129)
**ì˜í–¥ë„**: ë†’ìŒ  
**ë¬¸ì œ**: 
- `response.choices[0]` ì ‘ê·¼ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¼ ê²½ìš° IndexError ë°œìƒ
- `response.usage`ê°€ Noneì¼ ê²½ìš° AttributeError ë°œìƒ

**í˜„ì¬ ì½”ë“œ**:
```python
result = {
    "content": response.choices[0].message.content,
    "role": response.choices[0].message.role,
    "usage": {
        "prompt_tokens": response.usage.prompt_tokens,
        ...
    },
    ...
}
```

**ê¶Œì¥ ìˆ˜ì •**:
```python
if not response.choices or len(response.choices) == 0:
    raise GPTAPIError("API ì‘ë‹µì— choicesê°€ ì—†ìŠµë‹ˆë‹¤.")

choice = response.choices[0]
if not choice.message:
    raise GPTAPIError("API ì‘ë‹µì— messageê°€ ì—†ìŠµë‹ˆë‹¤.")

result = {
    "content": choice.message.content or "",
    "role": choice.message.role or "assistant",
    "usage": {
        "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
        "completion_tokens": response.usage.completion_tokens if response.usage else 0,
        "total_tokens": response.usage.total_tokens if response.usage else 0
    },
    "model": response.model,
    "finish_reason": choice.finish_reason
}
```

### 2. finish_reason ì²˜ë¦¬ ì—†ìŒ
**ì˜í–¥ë„**: ì¤‘ê°„  
**ë¬¸ì œ**: 
- `finish_reason`ì´ "length"ì¼ ê²½ìš° ì‘ë‹µì´ ì˜ë ¸ì„ ìˆ˜ ìˆìŒ
- "content_filter"ì¼ ê²½ìš° ì½˜í…ì¸  í•„í„°ë§ë¨
- ì´ëŸ¬í•œ ê²½ìš°ì— ëŒ€í•œ ê²½ê³ ë‚˜ ì²˜ë¦¬ ì—†ìŒ

**ê¶Œì¥ ìˆ˜ì •**:
```python
finish_reason = choice.finish_reason
if finish_reason == "length":
    logger.warning("ì‘ë‹µì´ max_tokens ì œí•œìœ¼ë¡œ ì˜ë ¸ìŠµë‹ˆë‹¤.")
elif finish_reason == "content_filter":
    logger.warning("ì‘ë‹µì´ ì½˜í…ì¸  í•„í„°ì— ì˜í•´ í•„í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤.")
```

### 3. RateLimitError ì¬ì‹œë„ ì „ëµ ê°œì„  í•„ìš”
**ì˜ì—­ë„**: ì¤‘ê°„  
**ë¬¸ì œ**: 
- RateLimitErrorì˜ ê²½ìš° ì‘ë‹µ í—¤ë”ì— `retry-after` ì •ë³´ê°€ ìˆì„ ìˆ˜ ìˆìŒ
- í˜„ì¬ëŠ” ì§€ìˆ˜ ë°±ì˜¤í”„ë§Œ ì‚¬ìš©í•˜ì—¬ ë¶ˆí•„ìš”í•œ ëŒ€ê¸° ì‹œê°„ ë°œìƒ ê°€ëŠ¥

**í˜„ì¬ ì½”ë“œ**:
```python
except RateLimitError as e:
    wait_time = self.retry_delay * (2 ** attempt)
    time.sleep(wait_time)
```

**ê¶Œì¥ ìˆ˜ì •**:
```python
except RateLimitError as e:
    # retry_after í—¤ë” í™•ì¸
    retry_after = getattr(e, 'retry_after', None)
    if retry_after:
        wait_time = float(retry_after)
    else:
        wait_time = self.retry_delay * (2 ** attempt)
    
    logger.warning(
        f"Rate Limit ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{self.max_retries}), "
        f"{wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„..."
    )
    time.sleep(wait_time)
    last_exception = e
```

### 4. íƒ€ì„ì•„ì›ƒ ì„¤ì • ì—†ìŒ
**ì˜ì—­ë„**: ì¤‘ê°„  
**ë¬¸ì œ**: 
- OpenAI í´ë¼ì´ì–¸íŠ¸ì— íƒ€ì„ì•„ì›ƒ ì„¤ì • ì—†ìŒ
- ì¥ì‹œê°„ ëŒ€ê¸° ê°€ëŠ¥

**ê¶Œì¥ ìˆ˜ì •**:
```python
from openai import OpenAI, Timeout

def __init__(self, ...):
    ...
    self.client = OpenAI(
        api_key=self.api_key,
        timeout=Timeout(connect=10.0, read=60.0, write=10.0, pool=10.0)
    )
```

### 5. í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  ì—†ìŒ
**ì˜ì—­ë„**: ë‚®ìŒ  
**ë¬¸ì œ**: 
- í† í° ì‚¬ìš©ëŸ‰ì„ ëˆ„ì  ì¶”ì í•˜ì§€ ì•ŠìŒ
- ë¹„ìš© ëª¨ë‹ˆí„°ë§ ì–´ë ¤ì›€

**ê¶Œì¥ ìˆ˜ì •**:
```python
class GPTClient:
    def __init__(self, ...):
        ...
        self.total_tokens_used = 0
        self.total_requests = 0
    
    def chat_completion(self, ...):
        ...
        result["usage"] = {...}
        self.total_tokens_used += result["usage"]["total_tokens"]
        self.total_requests += 1
        ...
```

### 6. ë¹ˆ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ ì—†ìŒ
**ì˜ì—­ë„**: ë‚®ìŒ  
**ë¬¸ì œ**: 
- `messages`ê°€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¼ ê²½ìš° API í˜¸ì¶œ ì‹¤íŒ¨
- ê²€ì¦ ì—†ìŒ

**ê¶Œì¥ ìˆ˜ì •**:
```python
def chat_completion(self, messages: List[Dict[str, str]], ...):
    if not messages:
        raise ValueError("messages ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    
    if not isinstance(messages, list):
        raise TypeError("messagesëŠ” ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    ...
```

### 7. temperature ë²”ìœ„ ê²€ì¦ ì—†ìŒ
**ì˜ì—­ë„**: ë‚®ìŒ  
**ë¬¸ì œ**: 
- `temperature`ê°€ 0.0~2.0 ë²”ìœ„ë¥¼ ë²—ì–´ë‚  ìˆ˜ ìˆìŒ
- OpenAI APIëŠ” ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ì—ëŸ¬ ë°œìƒ

**ê¶Œì¥ ìˆ˜ì •**:
```python
def chat_completion(self, ..., temperature: float = 0.7, ...):
    if not 0.0 <= temperature <= 2.0:
        raise ValueError(f"temperatureëŠ” 0.0~2.0 ë²”ìœ„ì—¬ì•¼ í•©ë‹ˆë‹¤: {temperature}")
    ...
```

### 8. max_tokens ê²€ì¦ ì—†ìŒ
**ì˜ì—­ë„**: ë‚®ìŒ  
**ë¬¸ì œ**: 
- `max_tokens`ê°€ ìŒìˆ˜ì´ê±°ë‚˜ ëª¨ë¸ ì œí•œì„ ì´ˆê³¼í•  ìˆ˜ ìˆìŒ

**ê¶Œì¥ ìˆ˜ì •**:
```python
def chat_completion(self, ..., max_tokens: Optional[int] = None, ...):
    if max_tokens is not None:
        if max_tokens <= 0:
            raise ValueError(f"max_tokensëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤: {max_tokens}")
        # ëª¨ë¸ë³„ ìµœëŒ€ í† í° ì œí•œ í™•ì¸ (ì„ íƒì )
    ...
```

### 9. ë¡œê¹… ê°œì„  í•„ìš”
**ì˜ì—­ë„**: ë‚®ìŒ  
**ë¬¸ì œ**: 
- ì„±ê³µ ì‹œ `debug` ë ˆë²¨ë§Œ ì‚¬ìš©
- ì¤‘ìš”í•œ API í˜¸ì¶œì€ `info` ë ˆë²¨ì´ ì ì ˆ

**ê¶Œì¥ ìˆ˜ì •**:
```python
logger.info(f"Chat Completion ì„±ê³µ: ëª¨ë¸={result['model']}, í† í°={result['usage']['total_tokens']}, finish_reason={result['finish_reason']}")
```

### 10. ì—ëŸ¬ ë©”ì‹œì§€ ê°œì„ 
**ì˜ì—­ë„**: ë‚®ìŒ  
**ë¬¸ì œ**: 
- ì¼ë¶€ ì—ëŸ¬ ë©”ì‹œì§€ê°€ ê¸°ìˆ ì ì„
- ì‚¬ìš©ì ì¹œí™”ì  ë©”ì‹œì§€ ë¶€ì¡±

---

## ğŸ” ì¶”ê°€ ê²€í†  ì‚¬í•­

### 1. ë¹„ìš© ìµœì í™”
- í† í° ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
- ëª¨ë¸ ì„ íƒ ìµœì í™”
- ìºì‹± ì „ëµ

### 2. ì„±ëŠ¥ ìµœì í™”
- ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
- ë°°ì¹˜ ì²˜ë¦¬
- ë³‘ë ¬ ìš”ì²­

### 3. ëª¨ë‹ˆí„°ë§
- API í˜¸ì¶œ ì„±ê³µë¥ 
- í‰ê·  ì‘ë‹µ ì‹œê°„
- ì—ëŸ¬ìœ¨ ì¶”ì 

---

## ğŸ“Š ì¢…í•© í‰ê°€

### ê°•ì 
1. âœ… ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ ë¡œì§
2. âœ… êµ¬ì²´ì ì¸ ì˜ˆì™¸ íƒ€ì… ì²˜ë¦¬
3. âœ… êµ¬ì¡°í™”ëœ ì‘ë‹µ ë°˜í™˜
4. âœ… í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ í¬í•¨
5. âœ… ì—°ê²° í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ

### ê°œì„  í•„ìš”
1. ğŸ”´ **ë†’ìŒ**: ì‘ë‹µ ì¸ë±ì‹± ì•ˆì „ì„±
2. ğŸŸ¡ **ì¤‘ê°„**: finish_reason ì²˜ë¦¬
3. ğŸŸ¡ **ì¤‘ê°„**: RateLimitError ì¬ì‹œë„ ì „ëµ ê°œì„ 
4. ğŸŸ¡ **ì¤‘ê°„**: íƒ€ì„ì•„ì›ƒ ì„¤ì •
5. ğŸŸ¢ **ë‚®ìŒ**: í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
6. ğŸŸ¢ **ë‚®ìŒ**: íŒŒë¼ë¯¸í„° ê²€ì¦ (messages, temperature, max_tokens)
7. ğŸŸ¢ **ë‚®ìŒ**: ë¡œê¹… ê°œì„ 
8. ğŸŸ¢ **ë‚®ìŒ**: ì—ëŸ¬ ë©”ì‹œì§€ ê°œì„ 

### ìš°ì„ ìˆœìœ„
- **ë†’ìŒ**: ì‘ë‹µ ì¸ë±ì‹± ì•ˆì „ì„±
- **ì¤‘ê°„**: finish_reason ì²˜ë¦¬, RateLimitError ì¬ì‹œë„ ì „ëµ, íƒ€ì„ì•„ì›ƒ ì„¤ì •
- **ë‚®ìŒ**: ë‚˜ë¨¸ì§€ ê°œì„  ì‚¬í•­

---

## ğŸ“ ê¶Œì¥ ìˆ˜ì • ì‚¬í•­

### ìˆ˜ì • 1: ì‘ë‹µ ì¸ë±ì‹± ì•ˆì „ì„±
```python
if not response.choices or len(response.choices) == 0:
    raise GPTAPIError("API ì‘ë‹µì— choicesê°€ ì—†ìŠµë‹ˆë‹¤.")

choice = response.choices[0]
if not choice.message:
    raise GPTAPIError("API ì‘ë‹µì— messageê°€ ì—†ìŠµë‹ˆë‹¤.")

result = {
    "content": choice.message.content or "",
    "role": choice.message.role or "assistant",
    "usage": {
        "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
        "completion_tokens": response.usage.completion_tokens if response.usage else 0,
        "total_tokens": response.usage.total_tokens if response.usage else 0
    },
    "model": response.model,
    "finish_reason": choice.finish_reason
}

# finish_reason ì²´í¬
if result["finish_reason"] == "length":
    logger.warning("ì‘ë‹µì´ max_tokens ì œí•œìœ¼ë¡œ ì˜ë ¸ìŠµë‹ˆë‹¤.")
elif result["finish_reason"] == "content_filter":
    logger.warning("ì‘ë‹µì´ ì½˜í…ì¸  í•„í„°ì— ì˜í•´ í•„í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤.")
```

### ìˆ˜ì • 2: RateLimitError ì¬ì‹œë„ ì „ëµ
```python
except RateLimitError as e:
    retry_after = getattr(e, 'retry_after', None)
    if retry_after:
        wait_time = float(retry_after)
    else:
        wait_time = self.retry_delay * (2 ** attempt)
    ...
```

### ìˆ˜ì • 3: íƒ€ì„ì•„ì›ƒ ì„¤ì •
```python
from openai import OpenAI, Timeout

self.client = OpenAI(
    api_key=self.api_key,
    timeout=Timeout(connect=10.0, read=60.0, write=10.0, pool=10.0)
)
```

### ìˆ˜ì • 4: íŒŒë¼ë¯¸í„° ê²€ì¦
```python
def chat_completion(self, messages: List[Dict[str, str]], temperature: float = 0.7, max_tokens: Optional[int] = None, **kwargs):
    if not messages:
        raise ValueError("messages ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    
    if not 0.0 <= temperature <= 2.0:
        raise ValueError(f"temperatureëŠ” 0.0~2.0 ë²”ìœ„ì—¬ì•¼ í•©ë‹ˆë‹¤: {temperature}")
    
    if max_tokens is not None and max_tokens <= 0:
        raise ValueError(f"max_tokensëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤: {max_tokens}")
    ...
```

---

## âœ… ê²€í†  ì™„ë£Œ

**ê²€í†  í•­ëª©**: `review_25_service_gpt_client`  
**ìƒíƒœ**: ì™„ë£Œ  
**ë‹¤ìŒ í•­ëª©**: `review_26_service_gpt_logger`

