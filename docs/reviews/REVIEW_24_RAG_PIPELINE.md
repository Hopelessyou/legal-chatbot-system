# RAG Pipeline ê²€í†  ë³´ê³ ì„œ

## ê²€í†  ëŒ€ìƒ
- íŒŒì¼: `src/rag/pipeline.py`
- ê²€í†  ì¼ì: 2024ë…„
- ê²€í†  ë²”ìœ„: ë¬¸ì„œ ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸, ë°°ì¹˜ ì²˜ë¦¬

---

## âœ… ì •ìƒ ë™ì‘ ë¶€ë¶„

### 1. íŒŒì´í”„ë¼ì¸ êµ¬ì¡° (Lines 16-30)
```python
class RAGIndexingPipeline:
    def __init__(self, collection_name: str = "rag_documents"):
        self.collection_name = collection_name
        self.collection = None
        self.parser = RAGDocumentParser()
        self.chunker = RAGChunker()
        self._initialize_collection()
```
- âœ… ëª…í™•í•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°
- âœ… Parser, Chunker, VectorDB ë¶„ë¦¬
- âœ… ì»¬ë ‰ì…˜ ì´ˆê¸°í™”

### 2. ë©”íƒ€ë°ì´í„° ì •ë¦¬ (Lines 32-63)
```python
@staticmethod
def _clean_metadata(metadata: Dict[str, Any]) -> Dict[str, Any]:
    """ChromaDB í˜¸í™˜ì„ ìœ„í•´ metadata ì •ë¦¬"""
    cleaned = {}
    for key, value in metadata.items():
        if value is None:
            continue
        elif isinstance(value, list):
            cleaned[key] = ", ".join(str(item) for item in value)
        elif isinstance(value, dict):
            cleaned[key] = json.dumps(value, ensure_ascii=False)
        ...
```
- âœ… ChromaDB í˜¸í™˜ì„± ì²˜ë¦¬
- âœ… None ê°’ ì œê±°
- âœ… ë¦¬ìŠ¤íŠ¸/ë”•ì…”ë„ˆë¦¬ ë³€í™˜

### 3. ë‹¨ì¼ ë¬¸ì„œ ì¸ë±ì‹± (Lines 65-122)
```python
def index_document(self, file_path: Path) -> int:
    # 1. íŒŒì‹±
    doc = self.parser.parse_document(file_path)
    # 2. ì²­í‚¹
    chunks = self.chunker.chunk_document(doc)
    # 3. ì„ë² ë”© ìƒì„±
    # 4. ë²¡í„° DB ì €ì¥
```
- âœ… ëª…í™•í•œ ë‹¨ê³„ë³„ ì²˜ë¦¬
- âœ… ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…

### 4. ë””ë ‰í† ë¦¬ ì¸ë±ì‹± (Lines 124-161)
```python
def index_directory(self, directory: Path, recursive: bool = True) -> int:
    # íŒŒì¼ ê²€ìƒ‰
    # ê° íŒŒì¼ ì¸ë±ì‹±
    # ì—ëŸ¬ ë°œìƒ ì‹œ ê³„ì† ì§„í–‰
```
- âœ… ì¬ê·€ì  ê²€ìƒ‰ ì§€ì›
- âœ… ê°œë³„ íŒŒì¼ ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰
- âœ… ì´ Chunk ê°œìˆ˜ ë°˜í™˜

### 5. ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (Lines 163-171)
```python
def clear_collection(self):
    """ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
    vector_db_manager.delete_collection(self.collection_name)
    self._initialize_collection()
```
- âœ… ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ë©”ì„œë“œ ì œê³µ

---

## âš ï¸ ë°œê²¬ëœ ë¬¸ì œì 

### 1. ë°°ì¹˜ ì²˜ë¦¬ ì—†ìŒ
**ì˜í–¥ë„**: ë†’ìŒ  
**ë¬¸ì œ**: 
- ê° Chunkë§ˆë‹¤ ê°œë³„ì ìœ¼ë¡œ ì„ë² ë”© ìƒì„± (Line 93)
- ëŒ€ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ ì‹œ ë¹„íš¨ìœ¨ì 
- OpenAI API í˜¸ì¶œ ë¹„ìš© ì¦ê°€

**í˜„ì¬ ì½”ë“œ**:
```python
for chunk in chunks:
    embedding_result = embedding_model.encode(chunk.content)
    embedding = embedding_result[0].tolist()
    ...
```

**ê¶Œì¥ ìˆ˜ì •**:
```python
# ëª¨ë“  Chunkì˜ Contentë¥¼ í•œ ë²ˆì— ì„ë² ë”© ìƒì„±
chunk_contents = [chunk.content for chunk in chunks]
embeddings_batch = embedding_model.encode(chunk_contents, batch_size=32)

for i, chunk in enumerate(chunks):
    if embeddings_batch.ndim == 2:
        embedding = embeddings_batch[i].tolist()
    else:
        embedding = embeddings_batch.tolist()
    ...
```

### 2. ì¤‘ë³µ ì¸ë±ì‹± ë°©ì§€ ì—†ìŒ
**ì˜í–¥ë„**: ì¤‘ê°„  
**ë¬¸ì œ**: 
- ë™ì¼í•œ ë¬¸ì„œë¥¼ ë‹¤ì‹œ ì¸ë±ì‹±í•˜ë©´ ì¤‘ë³µ Chunk ìƒì„±
- `chunk_id` ì¤‘ë³µ ì²´í¬ ì—†ìŒ

**ê¶Œì¥ ìˆ˜ì •**:
```python
def index_document(self, file_path: Path, overwrite: bool = False) -> int:
    # ê¸°ì¡´ Chunk í™•ì¸
    if not overwrite:
        existing_ids = self._get_existing_chunk_ids(file_path)
        if existing_ids:
            logger.warning(f"ë¬¸ì„œê°€ ì´ë¯¸ ì¸ë±ì‹±ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {file_path}")
            return len(existing_ids)
    ...
```

### 3. ë¹ˆ Chunk ì²˜ë¦¬ ì—†ìŒ
**ì˜ì—­ë„**: ì¤‘ê°„  
**ë¬¸ì œ**: 
- ë¹ˆ Contentë¥¼ ê°€ì§„ Chunkë„ ì¸ë±ì‹±
- ë¶ˆí•„ìš”í•œ ì„ë² ë”© ìƒì„± ë° ì €ì¥

**í˜„ì¬ ì½”ë“œ**:
```python
for chunk in chunks:
    embedding_result = embedding_model.encode(chunk.content)
    # ë¹ˆ Content ì²´í¬ ì—†ìŒ
```

**ê¶Œì¥ ìˆ˜ì •**:
```python
for chunk in chunks:
    if not chunk.content or not chunk.content.strip():
        logger.warning(f"ë¹ˆ Content Chunk ê±´ë„ˆëœ€: {chunk.chunk_id}")
        continue
    ...
```

### 4. ì»¬ë ‰ì…˜ None ì²´í¬ ì—†ìŒ (Line 110)
**ì˜ì—­ë„**: ì¤‘ê°„  
**ë¬¸ì œ**: 
- `self.collection`ì´ Noneì¼ ê²½ìš° AttributeError ë°œìƒ ê°€ëŠ¥

**ê¶Œì¥ ìˆ˜ì •**:
```python
def index_document(self, file_path: Path) -> int:
    if self.collection is None:
        raise RuntimeError("RAG ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    ...
```

### 5. íŒŒì¼ ê²½ë¡œ ê²€ì¦ ì—†ìŒ (Line 65)
**ì˜ì—­ë„**: ë‚®ìŒ  
**ë¬¸ì œ**: 
- íŒŒì¼ ì¡´ì¬ ì—¬ë¶€, íŒŒì¼ íƒ€ì… í™•ì¸ ì—†ìŒ

**ê¶Œì¥ ìˆ˜ì •**:
```python
def index_document(self, file_path: Path) -> int:
    if not file_path.exists():
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    
    if not file_path.is_file():
        raise ValueError(f"íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {file_path}")
    ...
```

### 6. ì§„í–‰ ìƒí™© ì¶”ì  ì—†ìŒ
**ì˜ì—­ë„**: ë‚®ìŒ  
**ë¬¸ì œ**: 
- ëŒ€ëŸ‰ íŒŒì¼ ì¸ë±ì‹± ì‹œ ì§„í–‰ ìƒí™© í™•ì¸ ë¶ˆê°€
- ì‚¬ìš©ì í”¼ë“œë°± ì–´ë ¤ì›€

**ê¶Œì¥ ìˆ˜ì •**:
```python
def index_directory(self, directory: Path, recursive: bool = True, progress_callback: Optional[Callable] = None) -> int:
    total_files = len(files)
    for idx, file_path in enumerate(files):
        if progress_callback:
            progress_callback(idx + 1, total_files, file_path)
        ...
```

### 7. ì—ëŸ¬ ë³µêµ¬ ì „ëµ ì—†ìŒ
**ì˜ì—­ë„**: ë‚®ìŒ  
**ë¬¸ì œ**: 
- ì¼ë¶€ Chunk ì¸ë±ì‹± ì‹¤íŒ¨ ì‹œ ì „ì²´ ë¡¤ë°± ì—†ìŒ
- ë¶€ë¶„ ì‹¤íŒ¨ ì²˜ë¦¬ ì „ëµ ì—†ìŒ

**ê¶Œì¥ ìˆ˜ì •**: íŠ¸ëœì­ì…˜ ë˜ëŠ” ì²´í¬í¬ì¸íŠ¸ ë©”ì»¤ë‹ˆì¦˜

### 8. ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ ì²˜ë¦¬ ë¶€ì¡±
**ì˜ì—­ë„**: ë‚®ìŒ  
**ë¬¸ì œ**: 
- ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ ì‹œ ì „ì²´ ë¬¸ì„œ ì¸ë±ì‹± ì‹¤íŒ¨
- ê°œë³„ Chunk ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰í•˜ëŠ” ì˜µì…˜ ì—†ìŒ

**ê¶Œì¥ ìˆ˜ì •**:
```python
for chunk in chunks:
    try:
        embedding_result = embedding_model.encode(chunk.content)
        ...
    except Exception as e:
        logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {chunk.chunk_id} - {str(e)}")
        if skip_on_error:
            continue
        else:
            raise
```

### 9. ë©”íƒ€ë°ì´í„° ì •ë¦¬ ë¡œì§ ì¤‘ë³µ
**ì˜ì—­ë„**: ë‚®ìŒ  
**ë¬¸ì œ**: 
- `_clean_metadata`ê°€ `VectorDBManager`ì—ë„ í•„ìš”í•  ìˆ˜ ìˆìŒ
- ì¤‘ë³µ ì½”ë“œ ê°€ëŠ¥ì„±

**ê¶Œì¥ ìˆ˜ì •**: ê³µí†µ ìœ í‹¸ë¦¬í‹°ë¡œ ë¶„ë¦¬

### 10. ë¡œê¹… ê°œì„  í•„ìš”
**ì˜ì—­ë„**: ë‚®ìŒ  
**ë¬¸ì œ**: 
- ì¸ë±ì‹± ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê¹… ì—†ìŒ
- ì²˜ë¦¬ ì‹œê°„, ì²˜ë¦¬ëŸ‰ ë“± ì •ë³´ ë¶€ì¡±

**ê¶Œì¥ ìˆ˜ì •**:
```python
import time

def index_document(self, file_path: Path) -> int:
    start_time = time.time()
    ...
    elapsed_time = time.time() - start_time
    logger.info(f"ì¸ë±ì‹± ì™„ë£Œ: {file_path.name} ({len(chunks)}ê°œ Chunk, {elapsed_time:.2f}ì´ˆ)")
    ...
```

---

## ğŸ” ì¶”ê°€ ê²€í†  ì‚¬í•­

### 1. ì„±ëŠ¥ ìµœì í™”
- ë°°ì¹˜ í¬ê¸° ìµœì í™”
- ë³‘ë ¬ ì²˜ë¦¬ (ë©€í‹°í”„ë¡œì„¸ì‹±)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”

### 2. ëª¨ë‹ˆí„°ë§
- ì¸ë±ì‹± ì§„í–‰ë¥ 
- ì„±ëŠ¥ ë©”íŠ¸ë¦­
- ì—ëŸ¬ìœ¨ ì¶”ì 

### 3. ì¬ì¸ë±ì‹± ì „ëµ
- ì¦ë¶„ ì¸ë±ì‹±
- ë³€ê²½ ê°ì§€
- ë²„ì „ ê´€ë¦¬

---

## ğŸ“Š ì¢…í•© í‰ê°€

### ê°•ì 
1. âœ… ëª…í™•í•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°
2. âœ… ë©”íƒ€ë°ì´í„° ì •ë¦¬ ë¡œì§
3. âœ… ë””ë ‰í† ë¦¬ ì¸ë±ì‹± ì§€ì›
4. âœ… ì—ëŸ¬ ë°œìƒ ì‹œ ê³„ì† ì§„í–‰

### ê°œì„  í•„ìš”
1. ğŸ”´ **ë†’ìŒ**: ë°°ì¹˜ ì²˜ë¦¬ ì¶”ê°€
2. ğŸŸ¡ **ì¤‘ê°„**: ì¤‘ë³µ ì¸ë±ì‹± ë°©ì§€
3. ğŸŸ¡ **ì¤‘ê°„**: ë¹ˆ Chunk ì²˜ë¦¬
4. ğŸŸ¡ **ì¤‘ê°„**: ì»¬ë ‰ì…˜ None ì²´í¬
5. ğŸŸ¢ **ë‚®ìŒ**: íŒŒì¼ ê²½ë¡œ ê²€ì¦
6. ğŸŸ¢ **ë‚®ìŒ**: ì§„í–‰ ìƒí™© ì¶”ì 
7. ğŸŸ¢ **ë‚®ìŒ**: ì—ëŸ¬ ë³µêµ¬ ì „ëµ
8. ğŸŸ¢ **ë‚®ìŒ**: ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ ì²˜ë¦¬
9. ğŸŸ¢ **ë‚®ìŒ**: ë©”íƒ€ë°ì´í„° ì •ë¦¬ ë¡œì§ ì¤‘ë³µ í•´ê²°
10. ğŸŸ¢ **ë‚®ìŒ**: ë¡œê¹… ê°œì„ 

### ìš°ì„ ìˆœìœ„
- **ë†’ìŒ**: ë°°ì¹˜ ì²˜ë¦¬ ì¶”ê°€
- **ì¤‘ê°„**: ì¤‘ë³µ ì¸ë±ì‹± ë°©ì§€, ë¹ˆ Chunk ì²˜ë¦¬, ì»¬ë ‰ì…˜ None ì²´í¬
- **ë‚®ìŒ**: ë‚˜ë¨¸ì§€ ê°œì„  ì‚¬í•­

---

## ğŸ“ ê¶Œì¥ ìˆ˜ì • ì‚¬í•­

### ìˆ˜ì • 1: ë°°ì¹˜ ì²˜ë¦¬ ì¶”ê°€
```python
def index_document(self, file_path: Path) -> int:
    ...
    chunks = self.chunker.chunk_document(doc)
    
    if not chunks:
        logger.warning(f"Chunkê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {file_path.name}")
        return 0
    
    # ë°°ì¹˜ë¡œ ì„ë² ë”© ìƒì„±
    chunk_contents = [chunk.content for chunk in chunks]
    embeddings_batch = embedding_model.encode(chunk_contents, batch_size=32)
    
    chunk_ids = []
    chunk_contents_list = []
    chunk_embeddings = []
    chunk_metadatas = []
    
    for i, chunk in enumerate(chunks):
        if not chunk.content or not chunk.content.strip():
            logger.warning(f"ë¹ˆ Content Chunk ê±´ë„ˆëœ€: {chunk.chunk_id}")
            continue
        
        if embeddings_batch.ndim == 2:
            embedding = embeddings_batch[i].tolist()
        else:
            embedding = embeddings_batch.tolist()
        
        cleaned_metadata = RAGIndexingPipeline._clean_metadata(chunk.metadata)
        
        chunk_ids.append(chunk.chunk_id)
        chunk_contents_list.append(chunk.content)
        chunk_embeddings.append(embedding)
        chunk_metadatas.append(cleaned_metadata)
    
    # ë²¡í„° DBì— ì €ì¥
    if chunk_ids:
        self.collection.add(
            ids=chunk_ids,
            embeddings=chunk_embeddings,
            documents=chunk_contents_list,
            metadatas=chunk_metadatas
        )
    ...
```

### ìˆ˜ì • 2: ë¹ˆ Chunk ì²˜ë¦¬
```python
for chunk in chunks:
    if not chunk.content or not chunk.content.strip():
        logger.warning(f"ë¹ˆ Content Chunk ê±´ë„ˆëœ€: {chunk.chunk_id}")
        continue
    ...
```

### ìˆ˜ì • 3: ì»¬ë ‰ì…˜ None ì²´í¬
```python
def index_document(self, file_path: Path) -> int:
    if self.collection is None:
        raise RuntimeError("RAG ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    ...
```

### ìˆ˜ì • 4: íŒŒì¼ ê²½ë¡œ ê²€ì¦
```python
def index_document(self, file_path: Path) -> int:
    if not file_path.exists():
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    
    if not file_path.is_file():
        raise ValueError(f"íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {file_path}")
    ...
```

### ìˆ˜ì • 5: ì§„í–‰ ìƒí™© ì¶”ì 
```python
def index_directory(self, directory: Path, recursive: bool = True, progress_callback: Optional[Callable] = None) -> int:
    ...
    for idx, file_path in enumerate(files):
        if progress_callback:
            progress_callback(idx + 1, len(files), file_path)
        try:
            chunks_count = self.index_document(file_path)
            total_chunks += chunks_count
        except Exception as e:
            logger.error(f"íŒŒì¼ ì¸ë±ì‹± ì‹¤íŒ¨: {file_path} - {str(e)}")
            continue
    ...
```

---

## âœ… ê²€í†  ì™„ë£Œ

**ê²€í†  í•­ëª©**: `review_24_rag_pipeline`  
**ìƒíƒœ**: ì™„ë£Œ  
**ë‹¤ìŒ í•­ëª©**: `review_25_service_gpt_client`

